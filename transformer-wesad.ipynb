{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6055182,"sourceType":"datasetVersion","datasetId":3445338}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# List to store individual subject DataFrames\ndata_frames = []\n\n# Iterate over the subject numbers\nfor subject_number in range(2, 18):\n    # Construct the file path for each subject\n    file_path = f'/kaggle/input/wesad-dataset/S{subject_number}_respiban.txt'\n    \n    # Check if the file exists\n    if os.path.exists(file_path):\n        # Read the file into a DataFrame\n        df = pd.read_csv(file_path, delimiter='\\t', skiprows=3, header=None)\n        # Select the first three columns\n        df_subset = df.iloc[:, 2]\n        # Append the DataFrame to the list\n        #print(df_subset.shape)\n \n        df_subset.columns = [f'Subject_{subject_number}']\n        \n        data_frames.append(df_subset)\n    else:\n        print(f'File not found for subject {subject_number}')\n\n# Concatenate all DataFrames into a single DataFrame\ndata_1 = pd.concat(data_frames,axis=1)\n\n# Print the shape of the combined data\nprint(data_1.shape)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-08T16:20:33.960747Z","iopub.execute_input":"2024-11-08T16:20:33.961051Z","iopub.status.idle":"2024-11-08T16:22:21.678484Z","shell.execute_reply.started":"2024-11-08T16:20:33.961016Z","shell.execute_reply":"2024-11-08T16:22:21.677528Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n# Define the constants\nchan_bit = 2 ** 16\nvcc = 3\n# Apply the equation to the dataset\ndata_wesad = data_1.applymap(lambda x: ((x / chan_bit - 0.5) * vcc) if not np.isnan(x) else np.nan)\n# Print the updated dataset\nprint(data_wesad)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-08T16:22:21.680078Z","iopub.execute_input":"2024-11-08T16:22:21.680411Z","iopub.status.idle":"2024-11-08T16:24:52.886361Z","shell.execute_reply.started":"2024-11-08T16:22:21.680376Z","shell.execute_reply":"2024-11-08T16:24:52.885355Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Select the data for the first subject\nsubject_data = data_wesad.iloc[:550, 4]  # Assuming the first column represents the first subject\n\n# Create a time axis for the plot\ntime_axis = range(550)\n\n# Plot the data\nplt.plot(time_axis, subject_data)\nplt.xlabel('Time')\nplt.ylabel('Voltage(mV)')\nplt.title('Plot of First 1000 Data Points - Subject 1')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-08T16:24:56.745643Z","iopub.execute_input":"2024-11-08T16:24:56.746021Z","iopub.status.idle":"2024-11-08T16:24:57.061248Z","shell.execute_reply.started":"2024-11-08T16:24:56.745983Z","shell.execute_reply":"2024-11-08T16:24:57.060236Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Specify the path to your .xlsx file\nfile_path = '/kaggle/input/wesad-dataset/WESAD_mins.xlsx'\n\n# Read the .xlsx file into a DataFrame\ndf = pd.read_excel(file_path)\ndf_sub = df.iloc[:, 1:]\n# Print the DataFrame\nprint(df_sub)","metadata":{"execution":{"iopub.status.busy":"2024-11-08T16:25:01.43916Z","iopub.execute_input":"2024-11-08T16:25:01.439642Z","iopub.status.idle":"2024-11-08T16:25:01.762359Z","shell.execute_reply.started":"2024-11-08T16:25:01.439591Z","shell.execute_reply":"2024-11-08T16:25:01.760899Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_intervals_base = 119\nnum_intervals_tsst = 59\n\nbase_interval = (df_sub['Base_end'] - df_sub['Base_start']) / (num_intervals_base + 1)\ntsst_interval = (df_sub['Tsst_end'] - df_sub['TSST_Start']) / (num_intervals_tsst + 1)\n\n# Create the new dataset\nnew_df = pd.DataFrame()\nnew_df['base_start'] = df_sub['Base_start']\nfor i in range(1, num_intervals_base + 1):\n    new_df[f'base_s{i}'] = df_sub['Base_start'] + i * base_interval\nnew_df['base_end'] = df_sub['Base_end']\nnew_df['tsst_start'] = df_sub['TSST_Start']\nfor i in range(1, num_intervals_tsst + 1):\n    new_df[f'tsst_s{i}'] = df_sub['TSST_Start'] + i * tsst_interval\nnew_df['tsst_end'] = df_sub['Tsst_end']\ndf_lebel=(new_df*700*60).T\n# Print the new dataset\nprint(df_lebel.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-08T16:25:20.226736Z","iopub.execute_input":"2024-11-08T16:25:20.227036Z","iopub.status.idle":"2024-11-08T16:25:20.351916Z","shell.execute_reply.started":"2024-11-08T16:25:20.227004Z","shell.execute_reply":"2024-11-08T16:25:20.351091Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"min_diffs = df_lebel.diff(axis=0).abs().min()\n\n# Find the absolute lowest value among the minimum differences\nabsolute_lowest = np.floor(min_diffs.min()).astype(int)\n\n# Print the absolute lowest value\nprint(absolute_lowest)","metadata":{"execution":{"iopub.status.busy":"2024-11-08T16:25:28.531711Z","iopub.execute_input":"2024-11-08T16:25:28.532344Z","iopub.status.idle":"2024-11-08T16:25:28.539175Z","shell.execute_reply.started":"2024-11-08T16:25:28.532286Z","shell.execute_reply":"2024-11-08T16:25:28.538201Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nsamples = []\nlebels = []\n# Iterate over the subjects\nfor subject_number in range(15):\n    # Read the first dataset for the current subject\n    df1 = data_wesad.iloc[:, subject_number] # Replace with your own file path\n    df2 = df_lebel.iloc[:,subject_number]\n    #print(subject_number)\n    indices = df2.values.astype(int).flatten()\n    # Iterate over the indices\n    for i in range(len(indices) - 1):\n        if i != 20:  # Exclude the 20th sample\n            start = indices[i]\n            end = start+absolute_lowest\n            sample = df1.iloc[start-1:end].values  # Cut the sample from the first dataset\n            sample = pd.Series(sample)\n            samples.append(sample)\n            #print(sample.shape)\n    lebel=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n           0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n           0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n           1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n           1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,]\n    lebels = lebels+lebel\n\n# Concatenate the samples along a new axis\nconcatenated = pd.concat(samples,axis=1).T\nlebel_all = pd.DataFrame(lebels)\n\n# Print the shape of the concatenated dataset\nprint(concatenated.shape)\nprint(lebel_all.shape)","metadata":{"execution":{"iopub.status.busy":"2024-11-08T16:25:34.552798Z","iopub.execute_input":"2024-11-08T16:25:34.553627Z","iopub.status.idle":"2024-11-08T16:25:34.988492Z","shell.execute_reply.started":"2024-11-08T16:25:34.553585Z","shell.execute_reply":"2024-11-08T16:25:34.987592Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# downsampling","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom scipy.signal import resample\noriginal_dataset = concatenated.T\n\n# Assuming original_dataset has shape (450, 40608)\ndownsampled_dataset = np.zeros((2700, 256 * (6768 // 700)))  # Downsampling to 256 Hz\nprint(original_dataset[0].shape)\nfor i in range(original_dataset.shape[1]):\n    original_signal = original_dataset[i]\n    downsampled_signal = resample(original_signal, 256 * (6768 // 700))\n    downsampled_dataset[i, :] = downsampled_signal\nwesad_x=downsampled_dataset.T\nprint(wesad_x.shape)  # Output: (90, 58418)","metadata":{"execution":{"iopub.status.busy":"2024-11-08T16:25:41.039215Z","iopub.execute_input":"2024-11-08T16:25:41.039552Z","iopub.status.idle":"2024-11-08T16:25:41.765879Z","shell.execute_reply.started":"2024-11-08T16:25:41.039518Z","shell.execute_reply":"2024-11-08T16:25:41.764932Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nprint(np.zeros((450,  40608 )).shape)","metadata":{"execution":{"iopub.status.busy":"2024-11-08T16:25:47.432273Z","iopub.execute_input":"2024-11-08T16:25:47.433147Z","iopub.status.idle":"2024-11-08T16:25:47.438083Z","shell.execute_reply.started":"2024-11-08T16:25:47.433105Z","shell.execute_reply":"2024-11-08T16:25:47.437087Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Select the data for the first subject\nwesad_x = pd.DataFrame(wesad_x)\nsubject_data = wesad_x.iloc[:2000, 4]  # Assuming the first column represents the first subject\n\n# Create a time axis for the plot\ntime_axis = range(2000)\n\n# Plot the data\nplt.plot(time_axis, subject_data)\nplt.xlabel('Time')\nplt.ylabel('Voltage(mV)')\nplt.title('Plot of First 1000 Data Points - Subject 1')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-08T16:25:49.957255Z","iopub.execute_input":"2024-11-08T16:25:49.957692Z","iopub.status.idle":"2024-11-08T16:25:50.245827Z","shell.execute_reply.started":"2024-11-08T16:25:49.957655Z","shell.execute_reply":"2024-11-08T16:25:50.244952Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X= wesad_x.T\ny=lebel_all","metadata":{"execution":{"iopub.status.busy":"2024-11-08T16:25:55.28541Z","iopub.execute_input":"2024-11-08T16:25:55.286145Z","iopub.status.idle":"2024-11-08T16:25:55.290798Z","shell.execute_reply.started":"2024-11-08T16:25:55.286104Z","shell.execute_reply":"2024-11-08T16:25:55.289844Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features=X\nlabels=y","metadata":{"execution":{"iopub.status.busy":"2024-11-08T16:25:59.100399Z","iopub.execute_input":"2024-11-08T16:25:59.100771Z","iopub.status.idle":"2024-11-08T16:25:59.274692Z","shell.execute_reply.started":"2024-11-08T16:25:59.100731Z","shell.execute_reply":"2024-11-08T16:25:59.273754Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# architecture Trasnformer","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import LayerNormalization, MultiHeadAttention, Add, Dense, Input, Conv1D, MaxPooling1D, BatchNormalization, Dropout, GlobalMaxPooling1D, Concatenate, Layer\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.regularizers import l2\nimport numpy as np\nfrom tensorflow.keras.layers import Bidirectional, LSTM\nfrom keras.layers import Conv1D, MaxPooling1D, BatchNormalization, Add, Concatenate, ZeroPadding1D\nfrom tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, BatchNormalization, SeparableConv1D, Bidirectional, LSTM, Dense, Add, Concatenate, GlobalAveragePooling1D, Reshape, Attention\nfrom tensorflow.keras.layers import LeakyReLU\nfrom tensorflow.keras.layers import GlobalAveragePooling1D, Dense, Multiply\nfrom tensorflow.keras.layers import SeparableConv1D, AveragePooling1D, BatchNormalization, Concatenate\n\ndef se_block(input_layer, reduction_ratio=4):\n    filters = input_layer.shape[-1]\n    se = GlobalAveragePooling1D()(input_layer)\n    se = Dense(filters // reduction_ratio, activation='relu')(se)\n    se = Dense(filters, activation='sigmoid')(se)\n    return Multiply()([input_layer, se])\n\nclass PositionalEncoding(Layer):\n    def __init__(self, position, d_model):\n        super(PositionalEncoding, self).__init__()\n        self.position = position\n        self.d_model = d_model\n\n    def get_angles(self, pos, i, d_model):\n        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n        return pos * angle_rates\n\n    def call(self, inputs):\n        seq_length = tf.shape(inputs)[1]\n        angles = self.get_angles(np.arange(self.position)[:, np.newaxis],\n                                 np.arange(self.d_model)[np.newaxis, :],\n                                 self.d_model)\n\n        # Apply sine to even indices in the array; 2i\n        angles[:, 0::2] = np.sin(angles[:, 0::2])\n\n        # Apply cosine to odd indices in the array; 2i+1\n        angles[:, 1::2] = np.cos(angles[:, 1::2])\n\n        pos_encoding = angles[np.newaxis, ...]\n        pos_encoding = tf.cast(pos_encoding, dtype=tf.float32)\n\n        # Scale the input (optional, closer to original implementation)\n        inputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n\n        return inputs + pos_encoding[:, :seq_length, :]\n\n\n\ndef shared_cnn(input_layer, filters1, kernel_size1, filters2, kernel_size2, filters3, kernel_size3):\n    # First CNN block\n    conv1 = Conv1D(filters=filters1, kernel_size=kernel_size1, activation='relu')(input_layer)\n    # Apply SE block after conv2\n    conv1 = se_block(conv1)\n    maxpool1 = AveragePooling1D(pool_size=2)(conv1)\n    batch_norm1 = BatchNormalization()(maxpool1)\n    \n    # Second CNN block\n    conv2 = Conv1D(filters=filters2, kernel_size=kernel_size2, activation='relu')(batch_norm1)\n    # Apply SE block after conv2\n    conv2 = se_block(conv2)\n    maxpool2 = AveragePooling1D(pool_size=2)(conv2)\n    batch_norm2 = BatchNormalization()(maxpool2)\n    \n    conv3 = Conv1D(filters=filters3, kernel_size=kernel_size3, activation='relu')(batch_norm2)\n    maxpool3 = AveragePooling1D(pool_size=2)(conv3)\n    batch_norm3 = BatchNormalization()(maxpool3)\n    \n    return batch_norm3\n\n\ndef hybrid_depthwise_cnn(input_layer, filters1, kernel_size1, filters2, kernel_size2, filters3, kernel_size3):\n    conv1 = SeparableConv1D(filters=filters1, kernel_size=kernel_size1, activation='relu')(input_layer)\n    conv1 = se_block(conv1)\n    maxpool1 = AveragePooling1D(pool_size=2)(conv1)\n    batch_norm1 = BatchNormalization()(maxpool1)\n\n    conv2 = SeparableConv1D(filters=filters2, kernel_size=kernel_size2, activation='relu')(batch_norm1)\n    conv2 = se_block(conv2)\n    maxpool2 = AveragePooling1D(pool_size=2)(conv2)\n    batch_norm2 = BatchNormalization()(maxpool2)\n\n    conv3 = SeparableConv1D(filters=filters3, kernel_size=kernel_size3, activation='relu')(batch_norm2)\n    #conv3 = se_block(conv3)\n    maxpool3 = AveragePooling1D(pool_size=2)(conv3)\n    batch_norm3 = BatchNormalization()(maxpool3)\n\n    return batch_norm3\n\n\ndef transformer_encoder(input_layer, num_heads, units):\n    # Layer normalization before self-attention (optional change for Pre-Norm)\n    norm_input = LayerNormalization()(input_layer)\n    \n    # Self-Attention\n    attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=units)(norm_input, norm_input)\n    attention_output = Add()([input_layer, attention_output])  # Residual connection\n\n    # Layer normalization before FFN (optional change for Pre-Norm)\n    norm_attention = LayerNormalization()(attention_output)\n    \n    # Feed-forward neural network with expansion\n    ffn = Dense(4 * units, activation='relu')(norm_attention)  # Expansion\n    ffn_output = Dense(units)(ffn)  # Reduce back to `units`\n    ffn_output = Add()([attention_output, ffn_output])  # Residual connection\n    transformer_output = LayerNormalization()(ffn_output)\n\n    return transformer_output\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-08T16:26:02.72519Z","iopub.execute_input":"2024-11-08T16:26:02.725569Z","iopub.status.idle":"2024-11-08T16:26:14.538404Z","shell.execute_reply.started":"2024-11-08T16:26:02.725532Z","shell.execute_reply":"2024-11-08T16:26:14.537604Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"k_num = 15","metadata":{"execution":{"iopub.status.busy":"2024-11-08T16:26:16.787155Z","iopub.execute_input":"2024-11-08T16:26:16.787825Z","iopub.status.idle":"2024-11-08T16:26:16.792249Z","shell.execute_reply.started":"2024-11-08T16:26:16.787779Z","shell.execute_reply":"2024-11-08T16:26:16.791197Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features=X\nlabels=y","metadata":{"execution":{"iopub.status.busy":"2024-11-08T16:26:18.949134Z","iopub.execute_input":"2024-11-08T16:26:18.94983Z","iopub.status.idle":"2024-11-08T16:26:18.953711Z","shell.execute_reply.started":"2024-11-08T16:26:18.949788Z","shell.execute_reply":"2024-11-08T16:26:18.952615Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(features.shape)\nprint(labels.shape)","metadata":{"execution":{"iopub.status.busy":"2024-11-08T16:26:21.351449Z","iopub.execute_input":"2024-11-08T16:26:21.351831Z","iopub.status.idle":"2024-11-08T16:26:21.356778Z","shell.execute_reply.started":"2024-11-08T16:26:21.35179Z","shell.execute_reply":"2024-11-08T16:26:21.355799Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training with Data Augmentation","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.layers import MultiHeadAttention, Flatten\n\n\n# Initialize KFold with 8 splits\nkf = KFold(n_splits=(k_num), shuffle=False, random_state=None)\n\n# Initialize lists to store the history and results of each fold\nall_fold_histories = []\nall_fold_accuracies = []\n\n\nfold = 1\nfor train_index, test_index in kf.split(features):\n        \n    tf.keras.backend.clear_session()\n    print(f\"\\nTraining on Fold 2nd {fold}...\")\n    print(test_index)\n    # Split the data\n    X_train, X_test = features.T[train_index], features.T[test_index]\n    y_train, y_test = labels.T[train_index], labels.T[test_index]\n    print(X_train.shape)\n    print(X_test.shape)\n    # Standard scaling\n    X_train = X_train.T\n    X_test = X_test.T\n    scaling = StandardScaler()\n    X_train = scaling.fit_transform(X_train)\n    X_test = scaling.transform(X_test)\n\n    # Reshape y_train and y_test to 1D arrays for proper indexing\n    y_train = y_train.to_numpy().ravel()  # Convert to NumPy and flatten\n    y_test = y_test.to_numpy().ravel()    # Convert to NumPy and flatten\n#data augmentation\n    X_majority = X_train[y_train == 0]\n    X_minority = X_train[y_train == 1]\n\n    print(X_minority.shape)\n    print(X_majority.shape)\n    minority_indices = np.where(y_train == 1)[0]  # Indices where y_train == 1\n    X_minority_augmented = np.zeros((2*X_minority.shape[0], 2304))\n    print(X_minority_augmented.shape)\n    j=0\n    i=0\n#print(int(X_minority.shape[0]/2))\n    for i in range(int(X_minority.shape[0]/2)):\n        i = i*2\n    #print(i)\n        x1=X_minority[i,:]\n        x2=X_minority[i+1,:]\n        x_combined = np.concatenate((X_minority[i,:], X_minority[i+1,:]))\n    #print(x_combined.shape)\n        x3 = x_combined[768:3072]\n        x4 = x_combined[1792:4096]\n        \n        X_minority_augmented[j] = x1  #np.roll(x1, 2000)\n        j=j+1    \n        X_minority_augmented[j] = x2  #np.roll(x1, 2000)\n        j=j+1\n        X_minority_augmented[j] = x3  #np.roll(x1, 2000)\n        j=j+1\n    #x1=X_minority[i,:]\n        X_minority_augmented[j] = x4 #np.roll(x1, 2000)x1\n        j=j+1\n    #i=i+1\n    #print(i)\n#print(i)\n    print(X_minority_augmented.shape)\n    X_train_augmented = np.vstack((X_majority, X_minority_augmented))\n    y_train_augmented = np.hstack((np.zeros(X_majority.shape[0]), np.ones(X_minority_augmented.shape[0])))\n# shuffle the data\n\n    idx = np.random.permutation(X_train_augmented.shape[0])\n#print(idx)\n    X_train_augmented = X_train_augmented[idx]\n    y_train_augmented = pd.DataFrame(y_train_augmented[idx])\n    print(X_train_augmented.shape)\n#print(y_train_augmented[0])\n    X_train=X_train_augmented\n    y_train=y_train_augmented #augmentation\n    # Define early stopping\n\n# Input layer, this shape is 2304 as a few subjects had fewer data points. To match that this was choosen\n    input_layer = Input(shape=(2304, 1))\n\n# Apply shared CNNs to the input\n    shared_cnn_output2 = shared_cnn(input_layer, filters1=32, kernel_size1=14, filters2=64, kernel_size2=8, filters3=128, kernel_size3=5)\n# Apply hybrid CNN\n    hybrid_cnn_output2 = hybrid_depthwise_cnn(input_layer, filters1=32, kernel_size1=14, filters2=64, kernel_size2=8, filters3=128, kernel_size3=5)\n# Concatenate CNN outputs \n    combined_output2 = Add()([shared_cnn_output2, hybrid_cnn_output2])\n    combined_output2 = se_block(combined_output2)\n# Apply a Bidirectional LSTM layer after each CNN output\n    bilstm_output2 = Bidirectional(LSTM(units=64, return_sequences=True))(combined_output2)\n# Apply Positional Encoding after LSTM outputs\n    pos_encoder2 = PositionalEncoding(position=2560, d_model=128)(bilstm_output2)   # d_model matches LSTM output size\n# Apply Transformer Encoder to each Positional Encoding output\n    transformer_output2 = transformer_encoder(pos_encoder2, num_heads=4, units=128)\n\n    transformer_output2_2 = transformer_encoder(transformer_output2, num_heads=2, units=128)\n\n# Apply a second Transformer Encoder layer if needed\n#transformer_output1_2 = transformer_encoder(transformer_output1_1, num_heads=2, units=256)\n    transformer_output2_3 = transformer_encoder(transformer_output2_2, num_heads=2, units=128)\n    transformer_output2_4 = transformer_encoder(transformer_output2_3, num_heads=2, units=128)\n    transformer_output2_5 = transformer_encoder(transformer_output2_4, num_heads=2, units=128)\n    transformer_output2_6 = transformer_encoder(transformer_output2_5, num_heads=2, units=128)\n    transformer_output2_7 = transformer_encoder(transformer_output2_6, num_heads=2, units=128)\n    transformer_output2_8 = transformer_encoder(transformer_output2_7, num_heads=2, units=128)\n\n# Residual connection\n    residual_output2 = Add()([transformer_output2_4, bilstm_output2])\n# Apply Global Max Pooling to flatten the outputs\n    pooled_output2 = GlobalAveragePooling1D()(residual_output2)\n\n# Dense layers\n    dense1 = Dense(units=256, activation='relu', kernel_regularizer=l2(0.01))(pooled_output2)\n    batch_norm4 = BatchNormalization()(dense1)\n    drop1 = Dropout(0.4)(batch_norm4)\n    dense2 = Dense(units=128, activation='relu', kernel_regularizer=l2(0.01))(drop1)\n    batch_norm5 = BatchNormalization()(dense2)\n    drop2 = Dropout(0.4)(batch_norm5)\n# Output layer for binary classification\n    output = Dense(units=1, activation='sigmoid')(drop2)\n# Build and compile the model\n    model = Model(inputs=input_layer, outputs=output)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n    es = EarlyStopping(monitor='val_loss', mode='min', patience=80)\n\n    # Check shapes\n    print(f\"Fold {fold} - X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n\n    # Define a unique filename for each fold\n    checkpoint_filename = f'best_model_fold_3nd{fold}.keras'\n    mc = ModelCheckpoint(checkpoint_filename, monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n\n    # Train the model with early stopping and a unique checkpoint for each fold\n    try:\n        history = model.fit(X_train, y_train, epochs=100, batch_size=64,\n                            validation_data=(X_test, y_test), callbacks= [es, mc])\n\n        # Save the history and validation accuracy for this fold\n        all_fold_histories.append(history.history)\n        all_fold_accuracies.append(max(history.history['val_accuracy']))\n        \n        print(f\"Fold {fold} - Validation accuracy: {all_fold_accuracies[-1]}\")\n    except Exception as e:\n        print(f\"Error during training Fold {fold}: {e}\")\n    \n    tf.keras.backend.clear_session()\n    fold += 1\n\n# Print or analyze the results\nprint(\"\\nCross-validation accuracies for each fold:\", all_fold_accuracies)\nprint(\"Average cross-validation accuracy:\", np.mean(all_fold_accuracies))\n","metadata":{"execution":{"iopub.status.busy":"2024-11-08T16:26:48.875359Z","iopub.execute_input":"2024-11-08T16:26:48.875815Z","iopub.status.idle":"2024-11-08T16:27:40.492777Z","shell.execute_reply.started":"2024-11-08T16:26:48.875768Z","shell.execute_reply":"2024-11-08T16:27:40.491082Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prediction/evaluation","metadata":{}},{"cell_type":"code","source":"from keras import layers\nimport keras\nimport numpy as np\nimport tensorflow as tf\n\n@keras.saving.register_keras_serializable(package=\"Custom\", name=\"PositionalEncoding\")\nclass PositionalEncoding(layers.Layer):\n    def __init__(self, position, d_model, **kwargs):\n        super(PositionalEncoding, self).__init__(**kwargs)\n        self.position = position\n        self.d_model = d_model\n\n    def get_angles(self, pos, i, d_model):\n        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n        return pos * angle_rates\n\n    def call(self, inputs):\n        seq_length = tf.shape(inputs)[1]\n        angles = self.get_angles(np.arange(self.position)[:, np.newaxis],\n                                 np.arange(self.d_model)[np.newaxis, :],\n                                 self.d_model)\n\n        # Apply sine to even indices in the array; 2i\n        angles[:, 0::2] = np.sin(angles[:, 0::2])\n\n        # Apply cosine to odd indices in the array; 2i+1\n        angles[:, 1::2] = np.cos(angles[:, 1::2])\n\n        pos_encoding = angles[np.newaxis, ...]\n        pos_encoding = tf.cast(pos_encoding, dtype=tf.float32)\n\n        # Scale the input (optional, closer to original implementation)\n        inputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n\n        return inputs + pos_encoding[:, :seq_length, :]\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"position\": self.position,\n            \"d_model\": self.d_model,\n        })\n        return config\n","metadata":{"execution":{"iopub.status.busy":"2024-11-06T12:38:09.823428Z","iopub.execute_input":"2024-11-06T12:38:09.823809Z","iopub.status.idle":"2024-11-06T12:38:09.83668Z","shell.execute_reply.started":"2024-11-06T12:38:09.823766Z","shell.execute_reply":"2024-11-06T12:38:09.835897Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.models import load_model\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import cohen_kappa_score\n\nkappa_scores = []\n# Initialize KFold with 4 splits\nkf = KFold(n_splits=k_num, shuffle=False, random_state=None)\n\nfold = 1\nfor train_index, test_index in kf.split(features):\n    print(f\"\\nResult on Fold {fold}...\")\n\n    X_train, X_test = features.T[train_index], features.T[test_index]\n    y_train, y_test = labels.T[train_index], labels.T[test_index]\n    \n    print(X_train.shape)\n    print(X_test.shape)\n    # Standard scaling\n    X_train = X_train.T\n    X_test = X_test.T\n    scaling = StandardScaler()\n    X_train = scaling.fit_transform(X_train)\n    X_test = scaling.transform(X_test)\n\n    # Reshape y_train and y_test to 1D arrays for proper indexing\n    y_train = y_train.to_numpy().ravel()  # Convert to NumPy and flatten\n    y_test = y_test.to_numpy().ravel()    # Convert to NumPy and flatten\n\n\n    #print(fold)    # Load the best model for this fold to evaluate\n    model_fold = load_model(f'best_model_fold_3nd{fold}.keras', custom_objects={\"PositionalEncoding\": PositionalEncoding})\n\n        # Get the predicted probabilities for the test set\n    y_pred_probs = model_fold.predict(X_test)\n\n        # Convert predicted probabilities to binary class labels (0 or 1)\n    y_pred_classes = (y_pred_probs > 0.5).astype(int).flatten()\n\n        # Generate the confusion matrix\n    cm = confusion_matrix(y_test, y_pred_classes)\n    print(\"Confusion Matrix for Fold {}:\\n\".format(fold), cm)\n\n        # Print the classification report for precision, recall, f1-score\n    report = classification_report(y_test, y_pred_classes, target_names=['Class 0', 'Class 1'])\n    print(\"\\nClassification Report for Fold {}:\\n\".format(fold), report)\n\n    # Compute ROC curve and ROC area\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\n    roc_auc = auc(fpr, tpr)\n\n    # Plotting the ROC curve\n    plt.figure(figsize=(10, 6))\n    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (area = {roc_auc:.2f})')\n    plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')  # Diagonal line\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(f'Receiver Operating Characteristic for Fold {fold + 1}')\n    plt.legend(loc='lower right')\n    plt.grid()\n    plt.show()\n\n    # Optionally print AUC value\n    print(f'Fold {fold + 1} - AUC: {roc_auc:.2f}')\n    # Calculate and store Kappa for this fold\n    kappa = cohen_kappa_score(y_test, y_pred_classes)\n    kappa_scores.append(kappa)\n    print(f\"Kappa for this fold: {kappa}\")\n    fold += 1\n\n# Print or analyze the results\nprint(\"\\nCross-validation accuracies for each fold:\", all_fold_accuracies)\nprint(\"Average cross-validation accuracy:\", np.mean(all_fold_accuracies))\n\n# Average Kappa across all folds\nprint(\"Average Kappa across folds:\", np.mean(kappa_scores))\n","metadata":{"execution":{"iopub.status.busy":"2024-11-06T12:40:47.683231Z","iopub.execute_input":"2024-11-06T12:40:47.683955Z","iopub.status.idle":"2024-11-06T12:41:02.116928Z","shell.execute_reply.started":"2024-11-06T12:40:47.683913Z","shell.execute_reply":"2024-11-06T12:41:02.116066Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Number of folds in all_fold_histories:\", len(all_fold_histories))","metadata":{"execution":{"iopub.status.busy":"2024-11-06T12:41:11.252011Z","iopub.execute_input":"2024-11-06T12:41:11.252398Z","iopub.status.idle":"2024-11-06T12:41:11.257441Z","shell.execute_reply.started":"2024-11-06T12:41:11.252362Z","shell.execute_reply":"2024-11-06T12:41:11.256616Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_auc_score\n\n# Assuming all_fold_histories contains the history from each fold\nfor fold in range(1, k_num+1):  # Adjust the range based on your number of folds\n    history = all_fold_histories[fold - 1]  # Get the history for the current fold\n\n    # Plotting accuracy\n    plt.figure(figsize=(12, 6))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(history['accuracy'], label='Training Accuracy')\n    plt.plot(history['val_accuracy'], label='Validation Accuracy')\n    plt.title(f'Fold {fold} - Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n\n    # Plotting loss\n    plt.subplot(1, 2, 2)\n    plt.plot(history['loss'], label='Training Loss')\n    plt.plot(history['val_loss'], label='Validation Loss')\n    plt.title(f'Fold {fold} - Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    plt.suptitle(f'Fold {fold} Performance')\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-06T12:41:18.103446Z","iopub.execute_input":"2024-11-06T12:41:18.104417Z","iopub.status.idle":"2024-11-06T12:41:19.511489Z","shell.execute_reply.started":"2024-11-06T12:41:18.104361Z","shell.execute_reply":"2024-11-06T12:41:19.510515Z"},"trusted":true},"outputs":[],"execution_count":null}]}