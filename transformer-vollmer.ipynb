{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6688097,"sourceType":"datasetVersion","datasetId":3857265}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# Specify the directory where the files are located\ndirectory = \"/kaggle/input/vollmer-csv/vollmer_s0\"  # Replace with the actual directory path\n\n# Create an empty list to store the data from the CSV files\ndf = pd.DataFrame()\ndata = []\n# Loop through the desired file numbers (1 to 13) with two digits\nfor file_number in range(1, 14):\n    # Use string formatting to ensure two-digit file numbers\n    file_name = f\"{file_number:02d}.csv\"\n    #print(file_name)\n    file_path = directory + file_name\n    #print(file_path)\n    data = pd.read_csv(file_path)\n    third_column = data.iloc[:, 1] \n    #print(third_column.shape)\n    df = pd.concat([df, third_column], axis=1, ignore_index=True)\n    #print(df.shape)\n\n# Now, data_list contains the data from the 13 specific CSV files with two-digit file numbers.\n\nprint(df.shape)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n\n# Check for GPU and set memory growth\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(\"GPUs are available and memory growth is set\")\n    except RuntimeError as e:\n        print(e)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# Specify the directory where the files are located\ndirectory = \"/kaggle/input/vollmer-csv/vollmer_labels_s0\"  # Replace with the actual directory path\n\n# Create an empty list to store the data from the CSV files\ncut_points = pd.DataFrame()\n\n# Loop through the desired file numbers (1 to 13) with two digits\nfor file_number in range(1, 14):\n    # Use string formatting to ensure two-digit file numbers\n    file_name = f\"{file_number:02d}.csv\"\n    #print(file_name)\n    file_path = directory + file_name\n    #print(file_path)\n    cut = pd.read_csv(file_path)\n    #print(cut)\n    selected_columns = cut[['FAROS_Marker/Rest', 'FAROS_Marker/Walking', 'FAROS_Marker/2-Back','Manual/Running']]\n\n    #print(selected_columns3)\n    cut_points = pd.concat([cut_points, selected_columns], axis=0, ignore_index=True)\n\n# Now, data_list contains the data from the 13 specific CSV files with two-digit file numbers.\n\nprint(cut_points.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ecg = df[0]\n    #print(ecg)\ncut_point = cut_points.iloc[0]\ncut_point1 = cut_point[0]\necg = ecg.to_frame()\necg.reset_index(drop=True, inplace=True)\n#print(ecg.T)\nrest = ecg.iloc[cut_point1:cut_point1+76800]\nrest.reset_index(drop=True, inplace=True)\n#print(rest.T)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nresult = pd.DataFrame()\nlabels = []\nfor i in range(13):\n    ecg = df[i]\n    #print(ecg)\n    cut_point = cut_points.iloc[i]\n    cut_point1 = cut_point[0]\n    ecg = ecg.to_frame()\n \n    #print(cut_point[1])\n    rest = ecg.iloc[cut_point1:cut_point1+76800]\n    rest.reset_index(drop=True, inplace=True)\n    result = pd.concat([result, rest], axis=1, ignore_index=True)\n\n    #walking = ecg.iloc[cut_point[1]:cut_point[1]+76800]\n    #walking.reset_index(drop=True, inplace=True)\n    #result = pd.concat([result, walking], axis=1, ignore_index=True)\n\n    stress = ecg.iloc[cut_point[2]:cut_point[2]+76800]\n    stress.reset_index(drop=True, inplace=True)\n    result = pd.concat([result, stress], axis=1, ignore_index=True)\n\n    #running = ecg.iloc[cut_point[3]:cut_point[3]+76800]\n    #running.reset_index(drop=True, inplace=True)\n    #result = pd.concat([result, running], axis=1, ignore_index=True)\n    \n    label = [0,1]\n    labels = labels+label\n\n#print(result)\nprint(labels)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(result.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\noriginal_data = result\n# Assuming 'original_data' is your (76800, 26) DataFrame\nrows, columns = original_data.shape\n\n# Step 1: Initialize an empty list to store segments for each column\nsegment_list = []\n\n# Step 2: Loop through each column to create 30 segments for each\nsegments_per_column = 30\ndata_points_per_segment = rows // segments_per_column  # 76800 / 30 = 2560\n\nfor col in range(columns):\n    # Get the data points for the current column\n    column_data = original_data.iloc[:, col].values\n    \n    # Split this column's data into 30 segments, each of length 2560\n    segmented_column_data = column_data.reshape(segments_per_column, data_points_per_segment).T\n    segment_list.append(segmented_column_data)\n\n# Step 3: Stack all segmented columns horizontally to create the final (2560, 780) DataFrame\nfinal_df = pd.DataFrame(np.hstack(segment_list))\n\n# Step 4: Create the alternating 0s and 1s array for (780, 1) shape\nlabels = np.tile([0, 1], columns // 2) if columns % 2 == 0 else np.tile([0, 1], columns // 2 + 1)[:columns]\ny_df = pd.DataFrame(np.repeat(labels, segments_per_column).reshape(-1, 1))\n\n# Verify final shapes\nprint(\"Final DataFrame shape:\", final_df.shape)  # Should be (2560, 780)\nprint(\"Label DataFrame shape:\", y_df)#.shape)      # Should be (780, 1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Select the data for the first subject\nsubject_data = result.iloc[:200, 1]  # Assuming the first column represents the first subject\n\n# Create a time axis for the plot\ntime_axis = range(200)\n\n# Plot the data\nplt.plot(time_axis, subject_data)\nplt.xlabel('Time')\nplt.ylabel('Voltage(mV)')\nplt.title('Plot of First 1000 Data Points - Subject 1')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Select the data for the first subject\n#wesad_x = pd.DataFrame(concatenated_results.T)\nsubject_data = final_df.iloc[:200, 30]  # Assuming the first column represents the first subject\n\n# Create a time axis for the plot\ntime_axis = range(200)\n\n# Plot the data\nplt.plot(time_axis, subject_data)\nplt.xlabel('Time')\nplt.ylabel('Voltage(mV)')\nplt.title('Plot of First 1000 Data Points - Subject 1')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# filtring order = 2 \nimport numpy as np\n\n#features2 = concatenated_results.values\n# Convert the Pandas DataFrame 'features' to a NumPy array\n\nfiltered_data = final_df.T\nprint(filtered_data.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nrepeated_y = y_df\n\n# Verify the shape of the resulting array\nprint(repeated_y.shape) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\n# Assuming filtered_data and repeated_y (as y) are already defined\n# Convert repeated_y to a NumPy array before splitting\nrepeated_y = np.array(repeated_y)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import LayerNormalization, MultiHeadAttention, Add, Dense, Input, Conv1D, MaxPooling1D, BatchNormalization, Dropout, GlobalMaxPooling1D, Concatenate, Layer\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.regularizers import l2\nimport numpy as np\nfrom tensorflow.keras.layers import Bidirectional, LSTM\nfrom keras.layers import Conv1D, MaxPooling1D, BatchNormalization, Add, Concatenate, ZeroPadding1D\nfrom tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, BatchNormalization, SeparableConv1D, Bidirectional, GRU, LSTM, Dense, Add, Concatenate, GlobalAveragePooling1D, Reshape, Attention\nfrom tensorflow.keras.layers import LeakyReLU\nfrom tensorflow.keras.layers import GlobalAveragePooling1D, Dense, Multiply\nfrom tensorflow.keras.layers import SeparableConv1D, AveragePooling1D, BatchNormalization, Concatenate\n\ndef se_block(input_layer, reduction_ratio=4):\n    filters = input_layer.shape[-1]\n    se = GlobalAveragePooling1D()(input_layer)\n    se = Dense(filters // reduction_ratio, activation='relu')(se)\n    se = Dense(filters, activation='sigmoid')(se)\n    return Multiply()([input_layer, se])\n\nclass PositionalEncoding(Layer):\n    def __init__(self, position, d_model):\n        super(PositionalEncoding, self).__init__()\n        self.position = position\n        self.d_model = d_model\n\n    def get_angles(self, pos, i, d_model):\n        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n        return pos * angle_rates\n\n    def call(self, inputs):\n        seq_length = tf.shape(inputs)[1]\n        angles = self.get_angles(np.arange(self.position)[:, np.newaxis],\n                                 np.arange(self.d_model)[np.newaxis, :],\n                                 self.d_model)\n\n        # Apply sine to even indices in the array; 2i\n        angles[:, 0::2] = np.sin(angles[:, 0::2])\n\n        # Apply cosine to odd indices in the array; 2i+1\n        angles[:, 1::2] = np.cos(angles[:, 1::2])\n\n        pos_encoding = angles[np.newaxis, ...]\n        pos_encoding = tf.cast(pos_encoding, dtype=tf.float32)\n\n        # Scale the input (optional, closer to original implementation)\n        inputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n\n        return inputs + pos_encoding[:, :seq_length, :]\n\n\n\ndef shared_cnn(input_layer, filters1, kernel_size1, filters2, kernel_size2, filters3, kernel_size3):\n    # First CNN block\n    conv1 = Conv1D(filters=filters1, kernel_size=kernel_size1, activation='relu')(input_layer)\n    # Apply SE block after conv2\n    conv1 = se_block(conv1)\n    maxpool1 = AveragePooling1D(pool_size=2)(conv1)\n    batch_norm1 = BatchNormalization()(maxpool1)\n    \n    # Second CNN block\n    conv2 = Conv1D(filters=filters2, kernel_size=kernel_size2, activation='relu')(batch_norm1)\n    # Apply SE block after conv2\n    conv2 = se_block(conv2)\n    maxpool2 = AveragePooling1D(pool_size=2)(conv2)\n    batch_norm2 = BatchNormalization()(maxpool2)\n \n    conv3 = Conv1D(filters=filters3, kernel_size=kernel_size3, activation='relu')(batch_norm2)\n    #conv3 = se_block(conv3)\n    maxpool3 = AveragePooling1D(pool_size=2)(conv3)\n    batch_norm3 = BatchNormalization()(maxpool3)\n   \n    return batch_norm3\n\n\ndef hybrid_depthwise_cnn(input_layer, filters1, kernel_size1, filters2, kernel_size2, filters3, kernel_size3):\n    conv1 = SeparableConv1D(filters=filters1, kernel_size=kernel_size1, activation='relu')(input_layer)\n    conv1 = se_block(conv1)\n    maxpool1 = AveragePooling1D(pool_size=2)(conv1)\n    batch_norm1 = BatchNormalization()(maxpool1)\n\n    conv2 = SeparableConv1D(filters=filters2, kernel_size=kernel_size2, activation='relu')(batch_norm1)\n    conv2 = se_block(conv2)\n    maxpool2 = AveragePooling1D(pool_size=2)(conv2)\n    batch_norm2 = BatchNormalization()(maxpool2)\n\n    conv3 = SeparableConv1D(filters=filters3, kernel_size=kernel_size3, activation='relu')(batch_norm2)\n    #conv3 = se_block(conv3)\n    maxpool3 = AveragePooling1D(pool_size=2)(conv3)\n    batch_norm3 = BatchNormalization()(maxpool3)\n   \n    return batch_norm3\n\n\ndef transformer_encoder(input_layer, num_heads, units):\n    # Layer normalization before self-attention (optional change for Pre-Norm)\n    norm_input = LayerNormalization()(input_layer)\n    \n    # Self-Attention\n    attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=units)(norm_input, norm_input)\n    attention_output = Add()([input_layer, attention_output])  # Residual connection\n\n    # Layer normalization before FFN (optional change for Pre-Norm)\n    norm_attention = LayerNormalization()(attention_output)\n    \n    # Feed-forward neural network with expansion\n    ffn = Dense(4 * units, activation='relu')(norm_attention)  # Expansion\n    ffn_output = Dense(units)(ffn)  # Reduce back to `units`\n    ffn_output = Add()([attention_output, ffn_output])  # Residual connection\n    transformer_output = LayerNormalization()(ffn_output)\n\n    return transformer_output\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"k_num = 13","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n# Convert `repeated_y` to a NumPy array if it's a DataFrame or Series\nfrom tensorflow.keras.layers import MultiHeadAttention, Flatten\n\nif not isinstance(repeated_y, np.ndarray):\n    repeated_y = repeated_y.to_numpy()\n#print(repeated_y)\n# Initialize KFold with 8 splits\nkf = KFold(n_splits=(k_num), shuffle=False, random_state=None)\n\n# Initialize lists to store the history and results of each fold\nall_fold_histories = []\nall_fold_accuracies = []\n\nfold = 1\n\nfor train_index, test_index in kf.split(filtered_data):\n    #if fold not in [2, 3, 6, 7, 13]:\n        #fold += 1\n         #continue\n\n    tf.keras.backend.clear_session()\n    print(f\"\\nTraining on Fold 2nd {fold}...\")\n    print(test_index)\n    filtered = filtered_data.T\n    # Split the data\n    X_train, X_test = filtered[train_index].T, filtered[test_index].T\n    y_train, y_test = repeated_y[train_index], repeated_y[test_index]\n\n    print(X_train.shape)\n    print(X_test.shape)\n    y_train = y_train.ravel()  # Ensure y_train is a 1D array\n    y_test = y_test.ravel()    # Ensure y_test is a 1D array\n        \n    print(y_train.shape)\n    print(y_test)\n    \n    # Check the shape of X_train and X_test\n    print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n\n    # Standard scaling\n    scaling2 = StandardScaler()\n    X_train = scaling2.fit_transform(X_train)\n    X_test = scaling2.transform(X_test)  # Now X_test should match X_train's feature count\n    input_layer = Input(shape=(2560, 1))\n    shared_cnn_output2 = shared_cnn(input_layer, filters1=32, kernel_size1=14, filters2=64, kernel_size2=8, filters3=128, kernel_size3=5)\n# Apply hybrid CNN\n    hybrid_cnn_output2 = hybrid_depthwise_cnn(input_layer, filters1=32, kernel_size1=14, filters2=64, kernel_size2=8, filters3=128, kernel_size3=5)\n# Concatenate CNN outputs \n    combined_output2 = Add()([shared_cnn_output2, hybrid_cnn_output2])\n    combined_output2 = se_block(combined_output2)\n# Apply a Bidirectional LSTM layer after each CNN output\n    bilstm_output2 = Bidirectional(LSTM(units=64, return_sequences=True))(combined_output2)                                                                      \n# Apply Positional Encoding after LSTM outputs\n    pos_encoder2 = PositionalEncoding(position=2560, d_model=128)(bilstm_output2)   # d_model matches LSTM output size\n# Apply Transformer Encoder to each Positional Encoding output\n    transformer_output2 = transformer_encoder(pos_encoder2, num_heads=4, units=128)\n# Apply a second Transformer Encoder layer if needed\n    transformer_output2_2 = transformer_encoder(transformer_output2, num_heads=2, units=128)\n\n# Apply a second Transformer Encoder layer if needed\n#transformer_output1_2 = transformer_encoder(transformer_output1_1, num_heads=2, units=256)\n    transformer_output2_3 = transformer_encoder(transformer_output2_2, num_heads=2, units=128)\n    transformer_output2_4 = transformer_encoder(transformer_output2_3, num_heads=2, units=128)\n    transformer_output2_5 = transformer_encoder(transformer_output2_4, num_heads=2, units=128)\n    transformer_output2_6 = transformer_encoder(transformer_output2_5, num_heads=2, units=128)\n    transformer_output2_7 = transformer_encoder(transformer_output2_6, num_heads=2, units=128)\n    transformer_output2_8 = transformer_encoder(transformer_output2_7, num_heads=2, units=128)\n    transformer_output2_9 = transformer_encoder(transformer_output2_8, num_heads=2, units=128)\n\n# Residual connection\n    residual_output2 = Add()([transformer_output2_5, bilstm_output2])\n# Apply Global Max Pooling to flatten the outputs\n    pooled_output2 = GlobalAveragePooling1D()(residual_output2)\n# Concatenate the outputs of both \n# Dense layers\n    dense1 = Dense(units=256, activation='relu', kernel_regularizer=l2(0.01))(pooled_output2)\n    batch_norm4 = BatchNormalization()(dense1)\n    drop1 = Dropout(0.4)(batch_norm4)\n    dense2 = Dense(units=128, activation='relu', kernel_regularizer=l2(0.01))(drop1)\n    batch_norm5 = BatchNormalization()(dense2)\n    drop2 = Dropout(0.4)(batch_norm5)\n# Output layer for binary classification\n    output = Dense(units=1, activation='sigmoid')(drop2)\n# Build and compile the model\n    model = Model(inputs=input_layer, outputs=output)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n    es = EarlyStopping(monitor='val_loss', mode='min', patience=80)\n    \n    # Check shapes\n    print(f\"Fold {fold} - X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n\n    # Define a unique filename for each fold\n    checkpoint_filename = f'best_model_fold_3nd{fold}.keras'\n    mc = ModelCheckpoint(checkpoint_filename, monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n\n    # Train the model with early stopping and a unique checkpoint for each fold\n    try:\n        history = model.fit(X_train, y_train, epochs=150, batch_size=32,\n                            validation_data=(X_test, y_test), callbacks=[es, mc])\n\n        # Save the history and validation accuracy for this fold\n        all_fold_histories.append(history.history)\n        all_fold_accuracies.append(max(history.history['val_accuracy']))\n        \n        print(f\"Fold {fold} - Validation accuracy: {all_fold_accuracies[-1]}\")\n    except Exception as e:\n        print(f\"Error during training Fold {fold}: {e}\")\n    \n    tf.keras.backend.clear_session()\n    fold += 1\n\n# Print or analyze the results\nprint(\"\\nCross-validation accuracies for each fold:\", all_fold_accuracies)\nprint(\"Average cross-validation accuracy:\", np.mean(all_fold_accuracies))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras import layers\nimport keras\nimport numpy as np\nimport tensorflow as tf\n\n@keras.saving.register_keras_serializable(package=\"Custom\", name=\"PositionalEncoding\")\nclass PositionalEncoding(layers.Layer):\n    def __init__(self, position, d_model, **kwargs):\n        super(PositionalEncoding, self).__init__(**kwargs)\n        self.position = position\n        self.d_model = d_model\n\n    def get_angles(self, pos, i, d_model):\n        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n        return pos * angle_rates\n\n    def call(self, inputs):\n        seq_length = tf.shape(inputs)[1]\n        angles = self.get_angles(np.arange(self.position)[:, np.newaxis],\n                                 np.arange(self.d_model)[np.newaxis, :],\n                                 self.d_model)\n\n        # Apply sine to even indices in the array; 2i\n        angles[:, 0::2] = np.sin(angles[:, 0::2])\n\n        # Apply cosine to odd indices in the array; 2i+1\n        angles[:, 1::2] = np.cos(angles[:, 1::2])\n\n        pos_encoding = angles[np.newaxis, ...]\n        pos_encoding = tf.cast(pos_encoding, dtype=tf.float32)\n\n        # Scale the input (optional, closer to original implementation)\n        inputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n\n        return inputs + pos_encoding[:, :seq_length, :]\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"position\": self.position,\n            \"d_model\": self.d_model,\n        })\n        return config\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(filtered_data.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.models import load_model\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import cohen_kappa_score\n\nkappa_scores = []\n# Initialize KFold with 4 splits\nkf = KFold(n_splits=k_num, shuffle=False, random_state=None)\nprint(filtered_data.shape)\nfold = 1\nfor train_index, test_index in kf.split(filtered_data):\n    #if fold not in [2, 3, 6, 7, 13]:\n        #fold += 1\n        #continue\n    print(f\"\\nResult on Fold {fold}...\")\n\n    # Split the data\n    X_train1, X_test1 = filtered_data.T[train_index], filtered_data.T[test_index]\n    y_train, y_test = repeated_y[train_index], repeated_y[test_index]\n\n    # Standard scaling\n    scaling = StandardScaler()\n    X_train = scaling.fit_transform(X_train1.T)\n    X_test = scaling.transform(X_test1.T)\n\n    # Reshape y_train and y_test to 1D arrays for proper indexing\n    y_train = y_train.ravel()\n    y_test = y_test.ravel()\n\n    #print(fold)    # Load the best model for this fold to evaluate\n    model_fold = load_model(f'best_model_fold_3nd{fold}.keras', custom_objects={\"PositionalEncoding\": PositionalEncoding})\n\n        # Get the predicted probabilities for the test set\n    print(X_test.shape)\n\n    print(X_train.shape)\n\n    \n    y_pred_probs = model_fold.predict(X_test)\n\n        # Convert predicted probabilities to binary class labels (0 or 1)\n    y_pred_classes = (y_pred_probs > 0.5).astype(int).flatten()\n\n        # Generate the confusion matrix\n    cm = confusion_matrix(y_test, y_pred_classes)\n    print(\"Confusion Matrix for Fold {}:\\n\".format(fold), cm)\n\n        # Print the classification report for precision, recall, f1-score\n    report = classification_report(y_test, y_pred_classes, target_names=['Class 0', 'Class 1'])\n    print(\"\\nClassification Report for Fold {}:\\n\".format(fold), report)\n\n    # Compute ROC curve and ROC area\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\n    roc_auc = auc(fpr, tpr)\n\n    # Plotting the ROC curve\n    plt.figure(figsize=(10, 6))\n    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (area = {roc_auc:.2f})')\n    plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')  # Diagonal line\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(f'Receiver Operating Characteristic for Fold {fold + 1}')\n    plt.legend(loc='lower right')\n    plt.grid()\n    plt.show() \n    \n    kappa = cohen_kappa_score(y_test, y_pred_classes)\n    kappa_scores.append(kappa)\n    print(f\"Kappa for this fold: {kappa}\")\n    # Optionally print AUC value\n    print(f'Fold {fold + 1} - AUC: {roc_auc:.2f}')\n    \n    fold += 1\n\n# Print or analyze the results\nprint(\"\\nCross-validation accuracies for each fold:\", all_fold_accuracies)\nprint(\"Average cross-validation accuracy:\", np.mean(all_fold_accuracies))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Number of folds in all_fold_histories:\", len(all_fold_histories))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_auc_score\n\n# Assuming all_fold_histories contains the history from each fold\nfor fold in range(1, 13+1):  # Adjust the range based on your number of folds\n\n    history = all_fold_histories[fold - 1]  # Get the history for the current fold\n\n    # Plotting accuracy\n    plt.figure(figsize=(12, 6))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(history['accuracy'], label='Training Accuracy')\n    plt.plot(history['val_accuracy'], label='Validation Accuracy')\n    plt.title(f'Fold {fold} - Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n\n    # Plotting loss\n    plt.subplot(1, 2, 2)\n    plt.plot(history['loss'], label='Training Loss')\n    plt.plot(history['val_loss'], label='Validation Loss')\n    plt.title(f'Fold {fold} - Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    plt.suptitle(f'Fold {fold} Performance')\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}